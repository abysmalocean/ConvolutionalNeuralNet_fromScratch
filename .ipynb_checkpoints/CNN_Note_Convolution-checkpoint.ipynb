{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network implementation\n",
    "In CNN, the input image $X$ is of dimension $N\\times C \\times H \\times W$, the filter $F$ is of dimension $F\\times C \\times HH \\times WW$, the output $Y$ is of dimension $N\\times F\\times Hd \\times Wd$. In this note, for simplicity reason, we look at one input example ($N=1$), assume one color layer of input ($C=1$), only one filter ($F=1$), thus all the input, filter, output are 2d matrices.\n",
    "\n",
    "\n",
    "## Forward propogation\n",
    "$X$ is of dimension $H \\times W$, $F$ is of dimension $HH\\times WW$, if the padding size is $P$, the stride is $S$, then the output $Y$ has dimension $Hd\\times Wd$.\n",
    "$$\n",
    "Hd=1+(H+2P-HH)/S \\\\\\\n",
    "Wd=1+(W+2P-WW)/S\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_{kl}=\\sum_{i=0}^{HH-1} \\sum_{j=0}^{WW-1} F_{ij} X_{i+kS, j+lS}\n",
    "$$\n",
    "\n",
    "Or we can write this equation in another form\n",
    "$$\n",
    "Y_{kl}=\\sum_{m=kS}^{HH-1+kS} \\sum_{n=lS}^{WW-1+lS} F_{m-kS,n-lS} X_{mn}\n",
    "$$\n",
    "\n",
    "If we make a very simple neural network, without hidden layer and activation function. The loss function is defined as\n",
    "$$\n",
    "L=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f(Y_{kl})\n",
    "$$\n",
    "$f()$ is some defined function to calculate the total loss. \n",
    "\n",
    "Then the Gradient $L$ with respect to $X$ and $F$ are\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial F_{ij}}&=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f'(Y_{kl})\\frac{\\partial Y_{kl}}{\\partial F_{ij}} \\nonumber \\\\\\\n",
    "&=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f'(Y_{kl})X_{i+kS,j+lS} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial X_{mn}}&=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f'(Y_{kl})\\frac{\\partial Y_{kl}}{\\partial X_{mn}} \\nonumber \\\\\\\n",
    "&=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f'(Y_{kl})F_{m-kS,n-lS} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "in which, $f'(Y)$ is of dimension $Hd\\times Wd$, same as $Y$.\n",
    "\n",
    "## Where is convolution? Why $180^\\circ$ rotation?\n",
    "\n",
    "OK, for simplicity, let's assume there is no padding and stride is one ($P=0,S=1$).\n",
    "\n",
    "Then we get $Hd=1+H-HH, Wd=1+W-WW$\n",
    "\n",
    "$$\n",
    "Y_{kl}=\\sum_{i=0}^{HH-1} \\sum_{j=0}^{WW-1} F_{ij} X_{i+k, j+l}\n",
    "$$\n",
    "This is actually the cross-correlation of $F$ and $X$, $F\\star X$. Cross-correlation and convolution are quite similar to each other, in terms of calculation. We know that $\\bar F \\ast X = F \\star X$, where $\\bar F$ is the $180^\\circ$ rotation of $F$. $180^\\circ$ rotation is just another way to say 2D flip.\n",
    "\n",
    "If we flip the Filter right-left and up-down, $\\bar F_{HH-1-i,WW-1-j}=F_{ij}$, then after some rearrangement of the index, we get\n",
    "$$\n",
    "Y_{kl}=\\sum_{i=0}^{HH-1} \\sum_{j=0}^{WW-1} \\bar F_{ij} X_{k-i+HH-1, l-j+WW-1}\n",
    "$$\n",
    "Which is exactly 2D convolution, $\\bar F \\ast X$.\n",
    "And after careful check of the index of X, it never goes out of bound and exactly traverses from $0$ to $H-1$ for $k-i+HH-1$, $0$ to $W-1$ for $l-j+WW-1$, means that this convolution has mode of \"Valid\".\n",
    "\n",
    "For gradient, we also have\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{ij}}=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f'(Y_{kl})X_{i+k,j+l} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{mn}}=\\sum_k^{Hd-1} \\sum_l^{Wd-1} f'(Y_{kl})F_{m-k,n-l}\n",
    "$$\n",
    "\n",
    "It is clear to see that $\\frac{\\partial L}{\\partial F_{ij}} = \\bar f'(Y) \\ast X $ in \"Valid\" mode and $\\frac{\\partial L}{\\partial X_{mn}} = f'(Y) \\ast F $ in \"Full\" mode. $\\bar f'(Y)$ is the $180^\\circ$ rotation of $f(Y)$.\n",
    "\n",
    ">Note that when $S>1$, it is no longer standard cross-correlation of convolution. But I guess it might be a convention to call this method Convolutional Neural Network. \n",
    "\n",
    "## A better way to calculate gradient\n",
    "In calculating $\\frac{\\partial L}{\\partial X_{mn}}$, altough the equation is clear, the index of $F$ will go out of bound. For the out of bound index of $F$, we need to set them to be zeros. This can be done by some ```if else``` check, or just pad lots of zeros to $F$ to form a new filter. And it will not be easy to figure out correctly with padding $P>0$ and strike $S>1$.\n",
    "\n",
    "A trick to calculate $\\frac{\\partial L}{\\partial X_{mn}}$ is that, instead of fixing the index of $X$, then figuring out the index of $f'(Y)$ , we fix the index of $f'(Y)$, then figure out the index of $X$. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{i+kS,j+lS}}=\\sum_k^{Hd-1} \\sum_l^{Wd-1}f'(Y_{kl})F_{ij}\n",
    "$$\n",
    "The index $i,j$ will traverse the whole size of the filter $F$, and there is no need to concern about the index of $X$. This method is actually to \"Think backward\" of the forward propogation. This idea was inspired by discussion with Siyuan Wang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "class MyException(Exception):\n",
    "    pass\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  \"\"\"\n",
    "  Evaluate a numeric gradient for a function that accepts a numpy\n",
    "  array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    \n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    pos = f(x).copy()\n",
    "    x[ix] = oldval - h\n",
    "    neg = f(x).copy()\n",
    "    x[ix] = oldval\n",
    "    \n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "    it.iternext()\n",
    "  return grad\n",
    "\n",
    "def conv_forward(x, w, b, conv_param,method='native'):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "  The input consists of N data points, each with C channels, height H and width\n",
    "  W. We convolve each input with F different filters, where each filter spans\n",
    "  all C channels and has height HH and width HH.\n",
    "\n",
    "  Input:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - w: Filter weights of shape (F, C, HH, WW)\n",
    "  - b: Biases, of shape (F,)\n",
    "  - conv_param: A dictionary with the following keys:\n",
    "    - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "      horizontal and vertical directions.\n",
    "    - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "    H' = 1 + (H + 2 * pad - HH) / stride\n",
    "    W' = 1 + (W + 2 * pad - WW) / stride\n",
    "  - cache: (x, w, b, conv_param)\n",
    "  \"\"\"\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional forward pass.                           #\n",
    "  # Hint: you can use the function np.pad for padding.                        #\n",
    "  #############################################################################\n",
    "  N,C,H,W=x.shape\n",
    "  F,C,HH,WW=w.shape\n",
    "  S=conv_param['stride']\n",
    "  P=conv_param['pad']\n",
    "\n",
    "  x_pad=np.pad(x,((0,0),(0,0),(P,P),(P,P)),'constant')\n",
    "\n",
    "  Hout = 1 + int((H + 2 * P - HH) / S)\n",
    "  Wout = 1 + int((W + 2 * P - WW) / S)\n",
    "\n",
    "  out=np.empty((N,F,Hout,Wout))\n",
    "\n",
    "  if method=='native':\n",
    "    for i in range(N):\n",
    "      for j in range(F):\n",
    "        for k in range(Hout):\n",
    "          for l in range(Wout):\n",
    "            out[i,j,k,l]=np.sum(x_pad[i,:,k*S:HH+k*S,l*S:WW+l*S]*w[j,:,:,:])+b[j]\n",
    "\n",
    "  elif method=='conv':\n",
    "    if S!=1:\n",
    "      raise MyException(\"In conv method, stride S has to be 1!\")\n",
    "  # Below is to demonstrate that CNN forward is indeed doing convolve2d (only if stride = 1)\n",
    "    for i in range(N):\n",
    "      for j in range(F):\n",
    "        tmp=np.zeros((Hout,Wout))\n",
    "        for k in range(C):\n",
    "          tmp[:,:] +=signal.convolve2d(x_pad[i,k,:,:],w[j,k,::-1,::-1],'valid')\n",
    "        out[i,j,:,:]=tmp+b[j]\n",
    "  else:\n",
    "    raise MyException(\"Only native and conv methods are allowed.\")\n",
    "\n",
    "\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b, conv_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def conv_backward(dout, cache,method='native'):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives.\n",
    "  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x\n",
    "  - dw: Gradient with respect to w\n",
    "  - db: Gradient with respect to b\n",
    "  \"\"\"\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional backward pass.                          #\n",
    "  #############################################################################\n",
    "  x, w, b, conv_param = cache\n",
    "\n",
    "  N,C,H,W=x.shape\n",
    "  F,C,HH,WW=w.shape\n",
    "  S=conv_param['stride']\n",
    "  P=conv_param['pad']\n",
    "\n",
    "  x_pad=np.pad(x,((0,0),(0,0),(P,P),(P,P)),'constant')\n",
    "\n",
    "  _,_,Hout,Wout=dout.shape\n",
    "\n",
    "  # out=np.empty((N,F,Hout,Wout))\n",
    "  dx_pad=np.zeros(x_pad.shape)\n",
    "  dw=np.empty(w.shape)\n",
    "  db=np.empty(b.shape)\n",
    "\n",
    "  if method=='native':\n",
    "    for i in range(F):\n",
    "      for j in range(C):\n",
    "        for k in range(HH):\n",
    "          for l in range(WW):\n",
    "            dw[i,j,k,l]=np.sum(x_pad[:,j,k:k+S*Hout:S,l:l+S*Wout:S]*dout[:,i,:,:])\n",
    "      db[i]=np.sum(dout[:,i,:,:])\n",
    "\n",
    "    for i in range(N):\n",
    "      for j in range(F):\n",
    "        for k in range(Hout):\n",
    "          for l in range(Wout):\n",
    "            ## NOTE! here is the better way to calculate dx_pad\n",
    "            dx_pad[i,:,k*S:k*S+HH,l*S:l*S+WW] +=w[j,:,:,:]*dout[i,j,k,l]\n",
    "\n",
    "  elif method=='conv':\n",
    "    if S!=1:\n",
    "      raise MyException(\"In conv method, stride S has to be 1!\")\n",
    "  # using convolve2d, (only if stride = 1)\n",
    "    for j in range(F):\n",
    "      for k in range(C):\n",
    "        tmp=np.zeros((HH,WW))\n",
    "        for i in range(N):\n",
    "          tmp[:,:] +=signal.convolve2d(x_pad[i,k,:,:],dout[i,j,::-1,::-1],'valid')\n",
    "        dw[j,k,:,:]=tmp\n",
    "      db[j]=np.sum(dout[:,j,:,:])\n",
    "\n",
    "    for i in range(N):\n",
    "      for k in range(C):\n",
    "        tmp=np.zeros((H+2,W+2))\n",
    "        for j in range(F):\n",
    "          tmp[:,:] +=signal.convolve2d(w[j,k,:,:],dout[i,j,:,:],'full')\n",
    "        dx_pad[i,k,:,:]=tmp\n",
    "\n",
    "  else:\n",
    "    raise MyException(\"Only native and conv methods are allowed.\")\n",
    "\n",
    "\n",
    "  dx=dx_pad[:,:,P:H+P,P:W+P]\n",
    "\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward\n",
      "difference:  2.21214764175e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out_native, _ = conv_forward(x, w, b, conv_param,method='native')\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 1e-8\n",
    "print('Testing conv_forward')\n",
    "print('difference: ', rel_error(out_native, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward native VS convolve2d\n",
      "difference:  1.50175938895e-16\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "out_native, _ = conv_forward(x, w, b, conv_param,method='native')\n",
    "out_conv, _ = conv_forward(x, w, b, conv_param,method='conv')\n",
    "\n",
    "# The difference between results from conv and native method should be around float64 machine epsilon, 1e-16'\n",
    "print('Testing conv_forward native VS convolve2d')\n",
    "print('difference: ', rel_error(out_native, out_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward function\n",
      "dx error:  4.60052935193e-09\n",
      "dw error:  1.7534740975e-09\n",
      "db error:  1.77412698523e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 7, 7)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 4,4)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward(x, w, b, conv_param,method='native')[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward(x, w, b, conv_param,method='native')[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward(x, w, b, conv_param,method='native')[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward(x, w, b, conv_param,method='native')\n",
    "dx, dw, db = conv_backward(dout, cache,method='native')\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print('Testing conv_backward function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward function native VS convolve2d\n",
      "dx error:  6.7191408824e-15\n",
      "dw error:  1.21654565198e-15\n",
      "db error:  0.0\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 7, 7)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 7,7)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out, cache = conv_forward(x, w, b, conv_param,method='native')\n",
    "dx_conv, dw_conv, db_conv = conv_backward(dout, cache,method='conv')\n",
    "dx_native, dw_native, db_native = conv_backward(dout, cache,method='native')\n",
    "\n",
    "# The difference between results from conv and native method should be around float64 machine epsilon, 1e-16'\n",
    "print('Testing conv_backward function native VS convolve2d')\n",
    "print('dx error: ', rel_error(dx_conv, dx_native))\n",
    "print('dw error: ', rel_error(dw_conv, dw_native))\n",
    "print('db error: ', rel_error(db_conv, db_native))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pyqt4]",
   "language": "python",
   "name": "conda-env-pyqt4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
